Local Weighted Semi-supervised Discriminant Analysis for Dimensionality Reduction
Abstract
In this paper, we present a novel weighted version of semi-supervised discriminant analysis method by assigning weights to each labeled samples. The proposed within-class weight can detect the outliers and between-class weight can discover the support points in boundaries between different classes. In addition, our proposed method is robust to diverse-density classes and imbalanced boundaries. For high-dimensional dataset, our method can find a nice low-dimensional projection to preserve the discriminative information and manifold structure embedded in both labeled and unlabeled samples. It can also be easily kernelized to form a nonlinear method and do semi-supervised induction. The experiments show that our method can achieve very promising classification accuracies than other methods. 
Key words: Semi-supervised learning, Dimensionality Reduction, Discriminant Analysis, Weighted method

I. Introduction
Dealing with high-dimensional data has always been a major problem with the research of pattern recognition and machine learning. Typical applications of these include face recognition, document categorization, and image retrieval. Finding a low-dimensional representation of high-dimensional space, namely dimensionality reduction is thus of great practical importance. The goal of dimensionality reduction is to reduce the complexity of input space and embed high-dimensional space into a low-dimensional space while keeping most of the desired intrinsic information  [2/. Among all the dimensionality reduction techniques, Linear Discriminant Analysis  is the most popular method and has been widely used in many classification applications. The objective of LDA is to find the optimal projection that maximizing the between-class scatter matrix while minimizing the within-class scatter matrix. Provided that samples in each class lie in a linear subspace, LDA works well. But the results of LDA tend to be degraded when samples in a class forming several separated clusters, i.e. forming multi-modality making the densities in each cluster diverse. In order to deal with the problem of diverse-density classes, several works have been proposed for improving LDA [4-6] . By constructing an adjacent connection for nearby samples, the improved algorithms can guarantee the sample in one cluster has fewer connections with other samples in another cluster. The weight on each sample can also reflect the density around it enabling us to solve the diverse-density problem. In addition, the subspace discovered by these algorithms has no constraint of c-1 dimensionality as in LDA. Another drawback of LDA is that it has a strict constraint that the within-class scatter matrix must be nonsingular, otherwise the optimal projection cannot be found correctly. For many applications in which the number of dimensionality is larger than that of samples, the constraint cannot be satisfied. Many algorithms, which include PCA+LDA , Regularized LDA (RLDA) , Penalized LDA (PLDA) , has been proposed to solve this problem. This type of  algorithms has applied the idea of regularization by adding some constant values to the diagonal elements of within-class scatter matrix. It is easy to prove the within-class scatter matrix with regularization being positive definite, hence non-singular. In this paper, we will focus on the former problem.
In general, the above improved LDA methods are all supervised, which means they only use labeled information. Although these algorithms work pretty well, a considerable number of labeled samples are needed to achieve satisfactory results, or they tend to find subspace over-fitting to the labeled samples. But in some circumstances, it is difficult to guarantee sufficient labeled samples for training because labeling large number of data is time-consuming and costly. Conversely, unlabeled data may be abundant and can be easily obtained in the real world. Thus, semi-supervised learning methods [10-13], which incorporate both labeled and unlabeled data into learning process, can be used instead of relying only on supervised learning. On the other hand, graph based dimensionality reduction has attracted large amount of research interests during the last decade. Such algorithms include ISOMAP , Local Linear Embedding (LLE) , Local Tangent Space Alignment (LTSA) , Laplacian Eigenmaps (LE) , Locally Preserving Projection (LPP) , etc. These methods usually assume that the samples lie on a nonlinear manifold in high-dimensional space, and then look for a low-dimensional representation to preserve the embedded manifold. Though these methods are unsupervised, they can be applied to solve the problem of classification. But these algorithms have a limitation to extend to test samples. Recently, there are considerable interests and success on graph based semi-supervised dimensionality reduction [20-23]. These algorithms usually extend the conventional supervised methods by bringing the ideas from the above-mentioned unsupervised algorithms. They start from constructing a graph matrix incorporating neighborhood information of the input space and preserve the geometric information provided by both the labeled and unlabeled samples, as well as the discriminative information by labeled samples. One of the advantages of these algorithms is that they can be directly applied to the whole input space. In addition, these algorithms are transductive and can naturally extend to test samples, which is of great practical importance . 
In this paper, we propose a novel semi-supervised dimensionality reduction algorithm, called Local Weighted Semi-supervised Discriminant Analysis, which is motivated by improving Cai et al.  and Song et al.  in a weighted way. The algorithm of Cai et al., called Semi-supervised Discriminant Analysis (SDA) is an extended method of Regularized LDA (RLDA) . SDA has added a graph-based regularized term, called graph laplacian, to the original criteria of RLDA. The graph laplacian incorporates the prior knowledge provided by labeled and unlabeled samples to preserve the locality of data manifold. Thus, SDA is able to find a low-dimensional projection, which results in preserving the discriminative structure embedded in labeled samples as well as the intrinsic geometrical structure embedded in all samples. Song et al. built a unified framework for semi-supervised dimensionality reduction and SDA can be viewed as an instance in this framework. However, SDA cannot solve the problem when there are diverse-density classes or outliers, because estimating the within- and between- class scatter matrix of SDA is sensitive to the density distribution in a class. When the density distribution in a class is separated or when there are a considerable number of outliers, the accuracy of estimation will be degraded. In order to solve the problems, we propose a novel extended method to SDA, called Local Weighted Semi-supervised Discriminant Analysis by assigning a weight for each labeled sample instead of treating them equally. The weight can reflect the relative importance of labeled samples in a class making the detection of outliers possible. 
The contributions of this paper are 2 folded. First, as an extended method to SDA, our proposed method can find a nice low-dimensional projection to preserve the manifold structure and discriminative information embedded both in labeled and unlabeled samples. It can also be naturally extended to the kernel version using kernel trick  . The method is transductive, which is easy for performing semi-supervised induction to the test samples. Second, the proposed within-class weight can detect the outliers in a class while the between-class weight can discover the support points in boundary between different classes. In addition, our proposed method is robust to the diverse-density classes and imbalanced boundaries. As a result, our method can achieve accurate estimation of within-class and between-class scatter matrix, which is beneficial to the performance of classification. A case in point is that in the simulation of COIL-20 dataset, our method achieves a further improvement over SDA and other studied methods by at least 3%-4% in classification. For the USPS and Html-CityU1 dataset, we can achieve at least 1%-2% improvements over other methods. 
This paper is organized as follows: In Section 2 and 3, we firstly review the basic idea of SDA and propose our extended method. In Section 4, we introduce the formal definition of within-class weight and between-class weight. In Section 5, we will present the nonlinear version of our proposed method using kernel trick. Simulation results will be shown in Section 6. The final conclusions are drawn in Section 7. 

						      (21)
In a particular case where each weight is assigned by 1, the weighted within-class, between-class and total scatter matrix (S_w ) ?, (S_b ) ?, (S_t ) ? will become those in SDA. Thus, the weighted SDA will be equal to SDA.

IV. Formal definition of within-class weight and between-class weight
The goal of SDA is to find a low-dimensional projection preserving the discriminative structure embedded in labeled samples as well as the intrinsic geometrical structure embedded in all samples. However, SDA cannot solve the problem when there are diverse-density classes as the estimation of within-class and between-class scatter matrix is sensitive to the density distribution in a class. The accuracy of estimation tends to be degraded when there are diverse-density classes or outliers. In order to solve the problem, we propose a weighted version of SDA. By assigning weights for labeled sample instead of treating them equally, we can evaluate a labeled sample the importance in class making the detection of outliers possible. This can be achieved by the proposed within-class weight. In addition, we proposed between-class weight to calculate the weighted between-class scatter matrix. The between-class scatter matrix can discover the support points and solve the imbalanced-boundary problem. In this section, we will introduce the formal definitions of within-class weight and between-class weight.

	Within-class weight
Since we cannot guarantee there is only one cluster in a class, the within-class weight should reflect the importance of a labeled sample in the cluster. In other words, it means we need to identify how close a sample is to the cluster center. In order to handle the problem, we first construct a within-class neighborhood system based on pairwise neighborhoods. For two samples in the same class, if x_i is within k neighborhoods of x_j, x_j is also within k neighborhoods of x_i, we will build a connection between them. The within-class neighborhood system can connect the samples in the same cluster while disconnecting those belonging to different clusters, an example can be seen in Fig. 1(a). In addition, if the density distributions in different clusters are not the same, a case in point is in Fig. 1(b), the left-side blue points are sparser than the right-side ones, the weight should eliminate the effects of this multi-density problem. Thus, based on the above analysis, we define the within-class weight as 
where ¦Ã is a small value satisfied: ¦Ã?1, N_p^w is the set of k nearest neighborhoods of x_p (including x_p) with the same label, Density(x_p) is the density in x_p. The ratios between the density in x_p and those in its neighborhoods can eliminate the multi-density effects. Though the densities in two samples belonging to different clusters may be of great difference, the ratios between them and their respective neighborhoods, i.e. the relative densities can be close. Thus, when the density in x_p is larger than those in its neighborhoods, the ratios between them will be large. As a result, x_p will gain a high value of within-class weight, which means it is much close to its cluster center. In addition, if x_p has few neighborhoods with the same label, it means x_p is close to the boundary or it is an outlier. Thus in this case, the value of within-class weight will be low. In an extreme case that x_p has no neighborhoods with the same label, it will be set to a small value.
	Between-class weight
The within-class weight measures a labeled sample how close it is to its cluster center, while the between-class weight measures how close it is between a labeled sample and the boundary. The support points are the points nearest to the boundary. In order to find the support points, we first construct a between-class neighborhood system. The between-class neighborhood system can connect the samples nearest to the boundary. To be specific, for two samples belonging to different classes, if x_i is within k neighborhoods of x_j, x_j is also within k neighborhoods of x_i, we will build a connection between them. This can refer to an example in Fig. 1(c). Moreover, if the boundaries between different classes are imbalanced, for example in Fig. 1(d) the boundary between purple points and red points is shorter than that between purple points and blue points, the between-class weight should handle the imbalanced boundary problem. Thus, we define the between-class weight as 
w_w (x_p)={¡ö(¡Æ_(x_i¡ÊN_p^b)?(Density(x_i))/(Density(x_p))@¦Ã x_p  have no neighborhoods in N_p^b )©È						      (23)
where N_p^b is the set of k nearest neighborhoods of x_p belonging to different classes. The ratios between the density in x_p and those in its neighborhoods can handle the imbalanced boundary problem with the same reason in within-class weight. But different from the within-class weight defined in Eq. (22), the between-class weight chooses the reciprocal ratios between the densities of a sample and its neighborhoods, because the closer a support point is to the boundary, the lower it has the density. Thus, if the densities in the neighborhoods of a support point are larger than that in itself, the ratios between them will be large. Consequently, x_p will gain a high value of between-class weight, which means it is much close to the boundary. Moreover, if x_p has few neighborhoods belonging to different classes or even none, it means x_p is within its cluster and not close to the boundary. Thus in this case, the value of between-class weight will be low.
The density in a sample used in Eq. (22) (23) can be measured using a kernel based nearest neighbor estimation given by :

Fig. 1 Definition of with-class weight and between-class weight (a) The blue points in the left side and the right side belong to the same class. By building a within-class neighborhood system, the blue points in two sides cannot be connected as they belong to different clusters. The green lines are the connection edges. (b) The density of blue points in the left side is sparser than that in the right side. The hollow purple points are the outliers detected using the proposed within-class weight. (c) The between-class neighborhood system can connect the points nearest to the boundaries between different classes. The green lines are the connection edges in between-class neighborhood system. (d) The boundary between purple points and red points is shorter than that between purple points and blue points. The hollow green points are the support points discovered using the proposed between-class weight.

VI. Simulations
We test our proposed algorithm in four real-world applications including face recognition, object recognition, digit recognition and document categorization. In each application, we choose one representative dataset to test our method and other studied methods for comparisons. The compared methods include PCA, LDA, LPP and SDA. For LPP, we extend it to form a semi-supervised version denoted by S-LPP . The representative dataset chosen in each application are given by:
UMIST face dataset : The UMIST dataset consists of 564 grayscale images of 20 individuals. The images cover a wide range of poses from profile to frontal views for each individual. The size of each image is approximately 220¡Á220 with 256 gray levels per pixel. In this experiment, we down-sample each image to the size 32¡Á32.    
COIL-20 dataset : The COIL-20 dataset consists of 1440 grayscale images of 20 objects. Each object is rotated through 360 degrees and 72 images are taken: one at every 5 degrees of rotation. The size of each image is 128¡Á128 with 256 gray levels per pixel. In this experiment, we down-sample each image to the size 32¡Á32.
USPS dataset : The USPS dataset consists of 9298 images of 10 handwritten digits (0-9). The size of each image is 16¡Á16 with 256 gray levels per pixel. In this experiment, no other preprocessing is performed to the images.
Html_CityU1 dataset : The Html_CityU1 consists of 10400 documents of 26 categories. Each category contains 400 documents, which are retrieved from Google using a set of keywords. The set of keywords in a category is different from that in other categories, but some of the keywords may be shared among different categories. The keywords form the attributions in document vector. 
To test our method and other studied methods on the datasets, we split each dataset into the seen set and unseen set. The seen set is used to train the methods while the unseen set is used to test them. We then randomly select samples in the seen set to form labeled set and unlabeled set. For PCA and S-LPP, we use the seen set to train the methods. For LDA, we use labeled samples in the first l-c dimensional PCA subspace as input space to eliminate the null space of data covariance matrix and then to train the methods. For SDA and our proposed method, we use the seen set in the first l-c dimensional PCA subspace to train the methods. For kernel methods, we use the seen set to train the methods and no other preprocessing is performed. A nearest neighbor classifier is learned to test the unseen set by using the output space of labeled samples after dimensionality reduction.
Table 6 Data information and experimental settings

UMIST dataset for face recognition
In UMIST dataset, 15 samples per class are randomly selected as seen set and the remaining ones are as unseen set. For LPP, SDA and our proposed method, we adopt a Gaussian function to calculate the edge weight in neighborhood graph. The parameter k in the graph is set to 8 for simplicity. The variance ¦Ò in Gaussian function and the regularized parameter ¦Ë_i in Eq. (8) (21) are gained by 5-fold cross validation. For our proposed method, the parameter k in the within-class and between-class neighborhood system is simply set to 5. In the following simulations, each accuracy result of unseen set is an average over 20 random splits.
First, we randomly select 7 samples per class as labeled samples and others as unlabeled ones to test the reduced subspace under different dimensionality. The number of dimensionality is changed from 1 to 40 with interval 1. We then fix the optimal dimensionality and vary the labeled number to test the unseen set. The labeled number is varied from 2 to 12 with interval 1. Fig. 2(a) shows the accuracy results of unseen set with best parameters under different dimensionality. Fig. 3(a) shows the results with optimal dimensionality and other parameters under varied labeled number. From the Fig. 2(a) we can see that our proposed method can overcome the critical limitation of c-1 dimensionality in LDA and SDA. In Fig. 3(a), the results show that our method outperforms S-LPP, LDA and PCA in face recognition but is competitive with SDA. This is probably because for UMIST dataset the density distribution in each class is relatively dense. In this case, the within-class weights cannot reflect the distinctions of labeled samples thus making our method not better than SDA.  
In addition, we test the kernel version of our proposed method and compare it to its linear method. For simplicity, we adopt a polynomial kernel 
K(x_i,x_j ) ¡¼=(1+(x_i^T x_j)/(|x_i ||x_j |))¡½^d									      (34)
where d is polynomial degree. The goal of using polynomial kernel is to avoid tuning the variance ¦Ò in Gaussian kernel, as it is sensitive to the parameter. Table 7 shows the accuracy results including both linear and nonlinear versions of our method and other methods. The labeled number is fixed to 4, 8 and 12. From the results we can see the performance of kernel version of our method is not better than that of linear version. This is probably because the UMIST dataset is linearly separable, for which the linear version may perform better than the nonlinear version.
COIL-20 dataset for object recognition
In COIL-20 dataset, 20 samples per class are randomly selected as seen set and the remaining ones are as unseen set. The parameters in COIL-20 dataset are set the same as in UMIST dataset. We first randomly select 12 samples per class as labeled samples to test the reduced subspace under different dimensionality. We then fix the optimal dimensionality and vary the labeled number to test the unseen set. The kernel version of our proposed method is also tested and compared to its linear method and other studied methods.  
Fig. 2(b) shows the accuracy results of unseen set with best parameters under different dimensionality. Fig. 3(b) shows the results with optimal dimensionality and other parameters under varied labeled number. Table 7 shows the accuracy result including both linear and nonlinear versions of our method and other methods.  Fig. 2(b) shows the same phenomenon that our method can overcome the limitation of c-1 dimensionality in LDA and SDA. In Fig. 3(b), we can see our proposed method achieves a further improvement over SDA and other methods. This is probably because COIL-20 dataset forms diverse-density class. In this case, SDA and other methods cannot solve the problem. By incorporating the information provided both by labeled and unlabeled samples into the within-class weight, our proposed method can deal with the problem and achieve satisfactory results. In addition, the results in Table 7 show the performance of kernel version of our method is not better than that of linear version due to the same reason that COIL-20 dataset may follow a linearly separable distribution, for which the kernel-based nonlinear method cannot well solve the linear problem. 
USPS dataset for digit recognition
In USPS handwritten digit dataset, 100 samples per class are randomly selected as seen set and others are as unseen set. The parameters are adjusted in the same way as in UMIST dataset and COIL-20 dataset. In order to preserve the local geometry embedded in the seen set, the parameter k in neighborhood graph is set to 12 instead of 8 in the above two dataset. For within-class and between-class neighborhood system in our method, the neighborhood parameter is set to 8 instead of 5. We first fix the labeled number to test reduced subspace to find the optimal dimensionality. The labeled number is set to 30 per class. We then fix the optimal dimensionality and vary the labeled number to test the unseen set. The labeled number is varied from 10 to 50 with an interval 5. 
Fig. 2(c) shows the accuracy results of unseen set with best parameters under different dimensionality. Fig. 3(c) shows the results with optimal dimensionality and other parameters under varied labeled number. Table 7 shows the accuracy result including both linear and nonlinear versions of our method and other methods. The labeled number is fixed to 10, 30 and 50. From the results in fig. 3(c), we can see that our proposed method outperforms over SDA and other methods. The reason for it is in USPS dataset, samples in a class form diverse-density class. This can be true as in real world a handwritten digit has many styles of writing. Each style can be viewed as a cluster making the densities in class diverse. The results show that our proposed method is able to solve the diverse-density problem while SDA and other methods cannot well handle it. In addition, in table 7 we can observe the kernel version of our method outperforms the linear version. The reason is that the USPS dataset is likely to be nonlinearly separable, which results in that the nonlinear version is better than linear version. 
Html_CityU1 dataset for document categorization
In Html_CityU1 dataset, 100 samples per class are randomly selected as seen set and others as unseen set. In the application of document categorization, an inner product distance is often applied toth optimal dimensionality and other parameters under varied labeled number. The labeled number is varied the same as in USPS dataset. Table 7 shows the accuracy including both linear and nonlinear versions of our method and other methods.
From the results in Fig. 3(d), we can see that our proposed method demonstrates further improvements over SDA and other methods. This is because the density distribution in Html_CityU1 dataset is sparse, where exist many diverse-density classes and outliers. Our method can eliminate the effects of diverse-density class while other methods cannot solve the problem. In addition, we can obtain the same phenomenon in Fig. 3(b), Fig. 3(c) and Fig. 3(d) that our proposed method uses fewer labeled samples to achieve the same accuracy of unseen set than other methods. The results in Table 7 also show that the nonlinear method performs better than the linear method, which indicates Html_CityU1 dataset is nonlinearly separable.

Table 7 Simulation results on the unseen set over 20 random splits in each dataset (mean¡Àstd%)
In this part, we analyze the performances of our proposed method in four datasets. From the results shown in Table 7, we can see that our proposed method outperforms other methods in COIL-20, USPS and Html-CityU1 dataset. A case in point is that in COIL-20 dataset with 12 labeled number per class, our method can achieve a further improvement of about 4% over SDA in classification, for S-LPP, LDA and PCA, the improvements are 2%, 4%, and 7%, respectively. In USPS dataset with 50 labeled number per class, our method can achieve approximately 1% improvement over SDA, the improvements over S-LPP, LDA and PCA are approximately 1%, 7%, and 9%, respectively. In Html-CityU1 dataset with 50 labeled number per class, the improvements over SDA, S-LPP, LDA and PCA are 1%, 4%, 2% and 5%, respectively. But for UMIST dataset, the improvement is not evident. This is probably because the performance of our method is related to the characteristics of the dataset. For some datasets such as UMIST dataset, in which the density distribution of each class is relatively dense, our method may not perform better than SDA and other methods. For other dataset such as COIL-20 and USPS where there exist diverse-density classes, our method can achieve further improvements. The same phenomenon can also be observed when the dataset is sparse with many outliers such as Html-CityU1 dataset. 

VII. Conclusions
In this paper, we propose a novel extended method to semi-supervised discriminant analysis by assigning weights for each labeled sample instead of treating them equally. The proposed within-class weight can detect the outliers in a class and the between-class weight can discover the support points in boundary between different classes. In addition, our proposed method is robust to the diverse-density classes and imbalanced boundaries. For high-dimensional dataset, it can find a nice low-dimensional projection to preserve the discriminative structure embedded in labeled samples as well as manifold structure embedded in both labeled and unlabeled samples. The experiments show that our method can achieve a very promising classification accuracy compared to SDA and other studied methods.
Future Work
My future work can be several folded:
	Extend the traditional SDA method to the 2D-SDA method. The 2D-SDA method can extracts the image features directly from 2D images matrices rather than 1D vectors so that the image matrices do not need to be transformed into vector. The advantages of 2D-SDA can form some variance matrix than original variance matrix thus requires less time to extract images features and achieves a better recognition rate.
	Kernelize the SDA method. Kernelizing the SDA method can be achieved by the standard kernel trick. However applying the kernel trick can be inconvenient since new mathematical formulas have to be derived and new implementations have to be done separately from the linear implementations. To overcome this drawback, we can use an alternative kernelization framework called KPCA trick, which does not require a user to derive a new mathematical formula or re-implement a kernelized algorithms. Also, the KPCA trick framework avoids troublesome problems such as singularity.
	Extend the SDA methods to realize feature selection. Since feature extraction is an extension of feature selection, we can derive a feature selection method from SDA. Also, we can build a general framework of graph based semi-supervised feature selection method. Many methods such as Laplacian Score and Fisher Score can be viewed as an instance to our framework. 
